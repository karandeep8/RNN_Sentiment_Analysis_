# Importing libraries
from google.colab import drive
import tensorflow as tf
import keras
from tensorflow.keras.models import *
from tensorflow.keras.layers import *
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np


drive.mount('/content/drive')

drive.mount("/content/drive")

x = ["I am very happy today", "No, I do not like the movie"]  # Input
y = [1, 0]  # Actual Output

# Create a tokenizer, configured to only take into account the 20 most common words
tokenizer = Tokenizer(num_words=20)  # This will assure that 20 frequent used words will be used for further processing
# Indexing to the words are provided based on their frequency, if the word is used too many times then it's index
# will be 1

# Build the word index
tokenizer.fit_on_texts(x)
# This method is used to build word index from the input text x
# It will identify which word should be considered and accordingly, vocabulary will be created

# Turns strings into lists of integer indices
sequences = tokenizer.texts_to_sequences(x)
# This method help us to convert the input text into series of numbers according to the index associated with the
# word in the vocabulary

# Show the vocabulory
word_index = tokenizer.word_index
print(tokenizer.word_index)
print('Found %s unique tokens.' % len(word_index))
# word_index is the property that will provide you the list of words those are present in the vocabulary
# Along with the number associated with it basically the index

print(sequences)

# Identify amx length of reviews
max_length = 0
for review_number in range(len(sequences)):
    numberofwords = len(sequences[review_number])
    if (numberofwords) > (max_length):
        max_length = numberofwords
print(max_length)

# Whenever we are dealing with the input text, size of the input text is not constant
# so we have to make sure that our input is in the same size
# hence it is important to identify the maximum length from the given input text

# Padding the sequence for shorter reviews
data = pad_sequences(sequences, maxlen=max_length)
y = np.asarray(y)
print(data)
print("Shape of data tensor:", data.shape)
print("Shape of label tensor:", y.shape)

# Load my word embeddings
import gensim

wordembeddings = gensim.models.KeyedVectors.load_word2vec_format(
    "/content/drive/MyDrive/archive/GoogleNews-vectors-negative300.bin", binary=True)

# Converting data to matrix of shape(12, 300) where 12 is number of words in vocalbourly +1 and 300 word vector
# 300 is the dimension provided by the word to wake vector and 12 unique words are there in our vocabulary
unique_words = len(word_index)
total_words = unique_words + 1
skipped_words = 0
embedding_dim = 300
embedding_matrix = np.zeros((total_words, embedding_dim))

for word, index in tokenizer.word_index.items():
    try:
        embedding_vector = wordembeddings[
            word]  # we are passing the word of our vocabulary and getting the corresponding vector
    except:
        skipped_words = skipped_words + 1  # if the word is not found in the word to wake vector then we are keeping the count of the skipped words
        pass
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector
print("Embedding Matrix shape : ", embedding_matrix.shape)

embedding_matrix[3] - wordembeddings.wv['very']
# we have printed the third row of embedding matrix and from this row we are removing the word embedding associated with the very



# create embedding layer
embedding_layer = Embedding(total_words, embedding_dim, weights=[embedding_matrix], input_length=max_length,
                            trainable=False)
# the embedding matrix that we have got of size 12*300 has to be loaded as input matrix to the model that we are
# going to build the first layer is embedding layer total_words=12 embedding_dim=300 weights=embedding_matrix
# trainable is indicating that dont update the weight during the training session beacuse these weights are already
# trained by the word to wake model



# define model
model = Sequential()
model.add(embedding_layer)  # we are not going to train this in our model
model.add(SimpleRNN(128, activation='relu', return_sequences=True))
model.add(SimpleRNN(256, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
print(model.summary())

# we have defined that our model is sequential
# the first layer present is embedding layer
# 128 and 256 is indicating the number of neurons
# we are using the simplernn in sequential manner that we have to specify the return sequnce for each layer as true expect output layer
# this means the output generated by any layer will become input for another layer
# last layer only include 1 neuron with activation function as sigmoid
# this single neuron is more than enough to specify whther the given sentence is postive or negative

from keras.optimizers import *

# Complie Network
opt = SGD(lr=0.01, decay=1e-6)
model.compile(loss="binary_crossentropy", optimizer=opt, metrics=['accuracy'])
# model.complir(loss="binary_crossentropy, optimizer='adam", metrics=['accuracy])
# fit network
model.fit(data, y, epochs=10, verbose=1)
